{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEREBRUM - iPython Notebook Demo (training)\n",
    "<br>\n",
    "Training of the fully-volumetric model introduced in:\n",
    "\n",
    "<b><font size=\"4\">[CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI](https://www.sciencedirect.com/science/article/pii/S1361841520300542)</font></b>\n",
    "\n",
    "\n",
    "Enjoy! ðŸ˜ƒ\n",
    "\n",
    "---\n",
    "\n",
    "This second notebook explains how to train a new instance of CEREBRUM.\n",
    "\n",
    "The training should require a total of 30-40 GBs of VRAM (depending on the size of the MR volume) when `batch_size = 1`. We trained the model exploiting a machine equipped with 4 Desktop GPUs (NVIDIA 1080Ti) - but similar performance can be achieved using a workstation with the required amount of VRAM (e.g., three-four NVIDIA 2080, two NVIDIA V100s, or one NVIDIA RTX 8000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview of CEREBRUM](../assets/method_overview.png)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the libraries the notebook will be making use of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<built-in function getcwd>\n"
    }
   ],
   "source": [
    "# specify the location of the library\n",
    "lib_path = '.'\n",
    "\n",
    "import os\n",
    "\n",
    "# garbage collection should prevent memory leaks during training exploiting generators\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, lib_path)\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# useful for logging\n",
    "from datetime import datetime\n",
    "\n",
    "# import the CEREBRUM library\n",
    "\n",
    "from cer3brum_lib.vol_losses import losses_dict\n",
    "from cer3brum_lib import utils\n",
    "\n",
    "os.chdir('/Users/xinhui.li/Documents/monkey-segmentation/CER3BRUM/src/cer3brum_lib')\n",
    "print(os.getcwd)\n",
    "from fullyvol import cer3brum\n",
    "os.chdir('/Users/xinhui.li/Documents/monkey-segmentation/CER3BRUM/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/Users/xinhui.li/Documents/monkey-segmentation/CER3BRUM/src'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of variables the user can tweak:\n",
    "\n",
    " - `data_dims` is a tuple containing the dimensions of the training and testing volumes.In our case, for instance, after the neck cropping, data are sized `(192, 256, 168)` (respectively, in sagittal, coronal, and longitudinal/axial view);\n",
    " \n",
    " - `num_segmask_labels` is an integer representing the number of different classes found in the ground truth data. In our work, as mentioned in the data section, we trained CEREBRUM to segment 8 different classes;\n",
    " \n",
    " - `dataset_name` is a string containing the name of the subfolder of `data` containing the dataset (training and validation;\n",
    " \n",
    " - `use_standardisation` is a boolean variable deciding whether or not input data should be z-scored (`True`) or not (`False`);\n",
    " \n",
    " - `anat_suffix` is a string/suffix identifying every anatomical volume (see the \"data\" Notebooks for a use case); \n",
    "\n",
    " - `segm_suffix` is a string/suffix identifying every ground truth volume (see the \"data\" Notebooks for a use case);\n",
    " \n",
    " - `num_epochs` is an integer specifying the number of epochs to train the model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple containing the dimension of each volume (tuple of three integers)\n",
    "data_dims = (256, 320, 320)\n",
    "\n",
    "# for sake of clarity in the following code, define:\n",
    "num_slices_sag = data_dims[0]\n",
    "num_slices_cor = data_dims[1]\n",
    "num_slices_lon = data_dims[2]\n",
    "\n",
    "# (int)\n",
    "num_segmask_labels = 7\n",
    "\n",
    "# (str)\n",
    "dataset_name = 'hcp'\n",
    "\n",
    "# (bool)\n",
    "use_standardisation = True\n",
    "\n",
    "# (str)\n",
    "anat_suffix = '_ANAT.nii.gz'\n",
    "\n",
    "# (str)\n",
    "segm_suffix = '_SEGM.nii.gz'\n",
    "\n",
    "# (int)\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition, other parameters can be specified:\n",
    "\n",
    "- `learning_rate` is a float specifying the learning rate;\n",
    "- `num_filters` is an integer specifying the number of filters for the first layer of the architecture;\n",
    "- `encoder_act_funct` is a string specifying the activation function for the contraction path (encoder);\n",
    "- `decoder_act_funct` is a string specifying the activation function for the expansion path (decoder);\n",
    "- `loss_funct` is a string specifying the loss function to minimize during the training;\n",
    "- `model` is a string specifying the model architecture;\n",
    "- `training_samples` is an integer defining the number of samples (of the training set) to train the model instance on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (float)\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# (int)\n",
    "num_filters = 48\n",
    "\n",
    "# (str - to choose from Keras activation functions: https://keras.io/activations/)\n",
    "encoder_act_funct = 'relu'\n",
    "\n",
    "# (str - to choose from Keras activation functions: https://keras.io/activations/)\n",
    "decoder_act_funct = 'relu'\n",
    "\n",
    "# (str - to choose between 'categorical_crossentropy', 'multiclass_dice_coeff',\n",
    "# 'weighted_multiclass_dice_coeff', 'tversky_loss', 'tanimoto_coefficient').\n",
    "loss_funct = 'categorical_crossentropy'\n",
    "\n",
    "# (str - to choose between 'strconv' and 'maxpool')\n",
    "model_arch = 'strconv'\n",
    "\n",
    "# (int)\n",
    "training_samples = 5\n",
    "\n",
    "# (int)\n",
    "val_samples = 1\n",
    "\n",
    "# (str)\n",
    "classification_act_funct = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to match your dataset location\n",
    "data_path = os.path.join('../data', dataset_name)\n",
    "\n",
    "# specify the folders containing the training set and the validation set\n",
    "training_path = os.path.join(data_path, 'training')\n",
    "validation_path = os.path.join(data_path, 'validation')\n",
    "\n",
    "# build a model dictionary (for sake of clarity of the user-defined parameters)\n",
    "models_dict = {'maxpool' : cer3brum.ThreeLevelsMaxPool,\n",
    "               'strconv' : cer3brum.ThreeLevelsStrConv,\n",
    "              }\n",
    "               \n",
    "names_dict = {'maxpool' : 'c3rebrum_maxpool_%df_%d'%(num_filters, training_samples),\n",
    "              'strconv' : 'c3rebrum_strconv_%df_%d'%(num_filters, training_samples),\n",
    "              }\n",
    "\n",
    "losses_names_dict = {'categorical_crossentropy'       : '_cc',\n",
    "                     'multiclass_dice_coeff'          : '_sum_dc',\n",
    "                     'average_multiclass_dice_coeff'  : '_avg_dc',\n",
    "                     'weighted_multiclass_dice_coeff' : '_wght_dc',\n",
    "                     'tversky_loss'                   : '_tsv',\n",
    "                     'tanimoto_coefficient'           : '_tc'}\n",
    "\n",
    "# the model name is function of the chosen hyperparameters\n",
    "model_name = names_dict[model_arch] + losses_names_dict[loss_funct] + '.h5'\n",
    "\n",
    "models_dir  = os.path.join('../output/models/', dataset_name)\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "logs_dir = os.path.join('../output/logs', dataset_name)\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "         \n",
    "model_path    = os.path.join(models_dir, model_name)\n",
    "\n",
    "# location of the \"_LOG.json\" file - where all the training param.s will be logged\n",
    "logfile_path  = os.path.join(logs_dir, model_name[0:-3] + '_LOG.json')\n",
    "\n",
    "# location of the \"_ARCH.txt\" file - where the output of Kerad model.summary() will be saved\n",
    "archfile_path = os.path.join(logs_dir, model_name[0:-3] + '_ARCH.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "After building the lists of volumes we're going to train and validate on, load the z-score matrices computed in the data notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training on 5 volumes, validating on 1 (hcp)\n"
    }
   ],
   "source": [
    "# training directory and validation directory structure (one anatomical + segmentation per subdir)\n",
    "training_subdir_list = list() \n",
    "training_volumes_path = list()\n",
    "training_subdir_list = sorted(os.listdir(training_path))\n",
    "training_set_size = len(training_subdir_list)\n",
    "for idx in training_subdir_list:\n",
    "    training_volumes_path.append(os.path.join(training_path, idx))\n",
    "\n",
    "validation_subdir_list = list() \n",
    "validation_volumes_path = list()\n",
    "validation_subdir_list = sorted(os.listdir(validation_path))[0:val_samples]\n",
    "validation_set_size = len(validation_subdir_list)\n",
    "for idx in validation_subdir_list:\n",
    "    validation_volumes_path.append(os.path.join(validation_path, idx))\n",
    "\n",
    "print('Training on %d volumes, validating on %d (%s)'%(training_set_size, validation_set_size, dataset_name))\n",
    "\n",
    "\n",
    "\n",
    "stdz_matrices_path = os.path.join('../output/zscoring/', dataset_name)\n",
    "\n",
    "# in this case, mean and std are computed on the first 250 training volumes of the dataset\n",
    "voxelwise_mean_path = os.path.join(stdz_matrices_path, 'voxelwise_mean.nii.gz')\n",
    "voxelwise_std_path = os.path.join(stdz_matrices_path, 'voxelwise_std.nii.gz')\n",
    "\n",
    "if not os.path.exists(voxelwise_mean_path) or not os.path.exists(voxelwise_std_path):\n",
    "    print(\"ERROR: voxelwise mean and standard deviation volumes not found.\")\n",
    "    print('Have you run the data notebook?')    \n",
    "    sys.exit(0)\n",
    "\n",
    "# to make the data suitable for pooling, reduce the last dimension so that it can be divided by 2^3\n",
    "voxelwise_mean = np.array(nib.load(voxelwise_mean_path).dataobj[:, :, :168]).astype(dtype = 'float32')\n",
    "voxelwise_std = np.array(nib.load(voxelwise_std_path).dataobj[:, :, :168]).astype(dtype = 'float32')\n",
    "\n",
    "# zero-padding is added by some pipelines at the borders, thus the voxelwise variability\n",
    "# would result zero. Change these values to 1 in order to mantain the zero in the division\n",
    "# (and not give rise to NaNs)\n",
    "voxelwise_std[voxelwise_std == 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train on an arbitrary number of volumes, we build a generator that handles the loading of such data dinamically (not saturating the RAM of the machine we're using). In addition, we define a couple of useful function (for sake of clarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check volume integrity (presence of NaN and inf could arise during the standardisation)\n",
    "def check_vol_integrity(input_vol, vol_name):\n",
    "\n",
    "    if np.sum(np.isnan(input_vol)) != 0:\n",
    "        print('WARNING: %d NaN(s) found in volume \"%s\"!'%(np.sum(np.isnan(input_vol)), vol_name))\n",
    "        sys.exit(0)\n",
    "        \n",
    "    if np.sum(np.isinf(input_vol)) != 0:\n",
    "        print('WARNING: %d inf(s) found in volume \"%s\"!'%(np.sum(np.isinf(input_vol)), vol_name))\n",
    "        sys.exit(0)\n",
    "\n",
    "## ----------------------------------------\n",
    "\n",
    "# volumes z-scoring \n",
    "def volume_zscoring(input_vol, voxelwise_mean, voxelwise_std):\n",
    "\n",
    "    # standardize each training volume\n",
    "    input_vol -= voxelwise_mean\n",
    "    input_vol /= voxelwise_std\n",
    "    \n",
    "    return input_vol\n",
    "\n",
    "## ----------------------------------------\n",
    "\n",
    "# create a generator that handles data loading dynamically in order to fit the entire dataset into RAM\n",
    "def vol_generator(vol_list, data_dims, num_segmask_labels, batch_size,\n",
    "                  use_standardisation, voxelwise_mean, voxelwise_std):\n",
    "    \n",
    "    # \"batch_size\" here is not the \"actual\" batch size (onto which the gradient is computed),\n",
    "    # but just the number of volumes to load dinamically at once\n",
    "    num_volumes = len(vol_list)\n",
    "    \n",
    "    anat_suffix = '_ANAT.nii.gz'\n",
    "    segm_suffix = '_SEGM.nii.gz'\n",
    "    \n",
    "    while 1: \n",
    "\n",
    "        shuffle(vol_list)\n",
    "\n",
    "        # if \"num_samples\" cannot be divided without remainder by \"batch_size\", some samples are discarded\n",
    "        # (at mostr batch_size - 1); if batch_size = 1 (as in the fully volumetric case) then all the dataset\n",
    "        # is loaded during the execution\n",
    "        for offset in range(0, num_volumes, batch_size):\n",
    "            \n",
    "            # define the batch that will be loaded when the yield instruction is executed (vol paths)\n",
    "            batch_volumes_list = vol_list[offset : offset+batch_size]\n",
    "\n",
    "            # init the list that will contain the samples\n",
    "            x_train_batch = np.zeros((batch_size, data_dims[0], data_dims[1], data_dims[2], 1), dtype=np.float)\n",
    "            y_train_batch = np.zeros((batch_size, data_dims[0], data_dims[1], data_dims[2], num_segmask_labels), dtype=np.float)\n",
    "\n",
    "            # for every sample in the selected chunk\n",
    "            for vol_num, vol in enumerate(batch_volumes_list):\n",
    "                \n",
    "                vol_name = vol.split('/')[-1]\n",
    "\n",
    "                mri_path = os.path.join(vol, vol_name + anat_suffix)\n",
    "                segmask_path = os.path.join(vol, vol_name + segm_suffix)  \n",
    "\n",
    "                # load the actual volume cropping to 168 in the last dimension to avoid problems during pooling\n",
    "                temp = np.array(nib.load(mri_path).dataobj[:, :, :]).astype(dtype = 'float32')\n",
    "\n",
    "                # check if everything is ok\n",
    "                if temp.shape != data_dims:\n",
    "                    print('\\n Warning: volume \"%s\" size mismatch: skipping to the next volume...'%(vol_name))\n",
    "                    continue\n",
    "                \n",
    "                if use_standardisation == True:\n",
    "                    temp = volume_zscoring(temp, voxelwise_mean, voxelwise_std)\n",
    "\n",
    "                check_vol_integrity(temp, vol_name)\n",
    "\n",
    "                x_train_batch[vol_num, :, :, :, 0] = temp\n",
    "\n",
    "                seg = np.array(nib.load(segmask_path).dataobj[:, :, :]).astype(dtype = 'uint8')\n",
    "        \n",
    "                # #  as we don't have many cases of white matter lesions (class 4) merge this class with white matter (class 3\n",
    "                # seg_vol_fixed = np.copy(seg)\n",
    "                # seg_vol_fixed[seg_vol_fixed==4] = 3\n",
    "\n",
    "                # # \"shift\" every class by one (as class 4 is now empty)\n",
    "                # seg_vol_fixed[seg_vol_fixed>4]-=1\n",
    "            \n",
    "        \n",
    "                y_train_batch[vol_num] = np_utils.to_categorical(seg, num_segmask_labels)\n",
    "\n",
    "            # final batch-wise shuffling (meaningful iff batch_size>1)\n",
    "            yield shuffle(x_train_batch, y_train_batch)\n",
    "\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the generator is defined, initialise it (dynamic loading of training data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 1\n",
    "\n",
    "train_generator = vol_generator(vol_list            = training_volumes_path,\n",
    "                                data_dims           = data_dims,\n",
    "                                num_segmask_labels  = num_segmask_labels,\n",
    "                                batch_size          = 1,\n",
    "                                use_standardisation = use_standardisation,\n",
    "                                voxelwise_mean      = voxelwise_mean,\n",
    "                                voxelwise_std       = voxelwise_std,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that validation volumes are less than training volumes in number, we choose to load them statically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(001/11) - loading validation volume AIA24 and its mask... Done.\n",
      "(002/11) - loading validation volume DCI24 and its mask... Done.\n",
      "(003/11) - loading validation volume EDE31 and its mask... Done.\n",
      "(004/11) - loading validation volume FLL19 and its mask... Done.\n",
      "(005/11) - loading validation volume KMA25 and its mask... Done.\n",
      "(006/11) - loading validation volume MBA01 and its mask... Done.\n",
      "(007/11) - loading validation volume MTN22 and its mask... Done.\n",
      "(008/11) - loading validation volume PMN01 and its mask... Done.\n",
      "(009/11) - loading validation volume SSA28 and its mask... Done.\n",
      "(010/11) - loading validation volume SWE24 and its mask... Done.\n",
      "(011/11) - loading validation volume TAS05 and its mask... Done.\n"
     ]
    }
   ],
   "source": [
    "# define the validation data as a 5D volumes: VOLNUMxDIM1xDIM2xDIM3xCHs\n",
    "x_val = np.zeros((validation_set_size, num_slices_sag, num_slices_cor, num_slices_lon, num_sequences),\n",
    "                 dtype=np.float)\n",
    "\n",
    "# define the validation GT as a 5D volumes: VOLNUMxDIM1xDIM2xDIM3xN_CLASSES\n",
    "y_val = np.zeros((validation_set_size, num_slices_sag, num_slices_cor, num_slices_lon, num_segmask_labels),\n",
    "                 dtype=np.uint8)\n",
    "\n",
    "vol_num = 0\n",
    "\n",
    "\n",
    "for vol in validation_volumes_path:\n",
    "\n",
    "    vol_name = vol.split('/')[-1]\n",
    "\n",
    "    mri_path = os.path.join(vol, vol_name + anat_suffix)\n",
    "    segmask_path = os.path.join(vol, vol_name + segm_suffix)\n",
    "\n",
    "    print('(%03d/%d) - loading validation volume %s and its mask...'%(vol_num+1, validation_set_size, vol_name)),\n",
    "\n",
    "    # to make the data suitable for pooling, reduce the last dimension so that it can be divided by 2^3\n",
    "    temp = np.array(nib.load(mri_path).dataobj[:, :, :168]).astype(dtype = 'float32')\n",
    "    \n",
    "    # check if everything is ok\n",
    "    if temp.shape != data_dims:\n",
    "        print('\\n Warning: volume \"%s\" size mismatch: skipping to the next volume...'%(vol_name))\n",
    "        continue\n",
    "\n",
    "    if use_standardisation == True:\n",
    "        temp = volume_zscoring(temp, voxelwise_mean, voxelwise_std)\n",
    "    \n",
    "    check_vol_integrity(temp, vol_name)\n",
    "\n",
    "    x_val[vol_num, :, :, :, 0] = temp\n",
    "\n",
    "\n",
    "    seg = np.array(nib.load(segmask_path).dataobj[:, :, :168]).astype(dtype = 'uint8')\n",
    "\n",
    "    #  as we don't have many cases of white matter lesions (class 4) merge this class with white matter (class 3\n",
    "    seg_vol_fixed = np.copy(seg)\n",
    "    seg_vol_fixed[seg_vol_fixed==4] = 3\n",
    "    \n",
    "    # \"shift\" every class by one (as class 4 is now empty)\n",
    "    seg_vol_fixed[seg_vol_fixed>4]-=1\n",
    "    \n",
    "    y_val[vol_num] = np_utils.to_categorical(seg_vol_fixed, num_segmask_labels)\n",
    "    \n",
    "    vol_num = vol_num + 1\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "# to check if the class merge operation worked out as intended, make sure each voxel was\n",
    "# assigned to exactly one class\n",
    "assert np.min(np.sum(y_val, axis = 4)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialise the model, define some callbacks, and take care of some of the logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved at the following location: ../output/models/glasgow/3T/c3rebrum_strconv_48f_900_cc.h5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 192, 256, 168 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_lvl1a_conv (Conv3D)         (None, 192, 256, 168 1344        main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_1to2 (Conv3D)         (None, 48, 64, 42, 9 295008      enc_lvl1a_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "enc_lvl2a_conv (Conv3D)         (None, 48, 64, 42, 9 248928      down_conv_1to2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "enc_lvl2b_conv (Conv3D)         (None, 48, 64, 42, 9 248928      enc_lvl2a_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_2to3 (Conv3D)         (None, 24, 32, 21, 1 147648      enc_lvl2b_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_a_conv (Conv3D)      (None, 24, 32, 21, 1 995520      down_conv_2to3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_b_conv (Conv3D)      (None, 24, 32, 21, 1 995520      bottleneck_a_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_c_conv (Conv3D)      (None, 24, 32, 21, 1 995520      bottleneck_b_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_3to2 (Conv3DTranspose)  (None, 48, 64, 42, 9 147552      bottleneck_c_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lvl2_longskip (Add)             (None, 48, 64, 42, 9 0           enc_lvl2b_conv[0][0]             \n",
      "                                                                 up_conv_3to2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dec_lvl2a_conv (Conv3D)         (None, 48, 64, 42, 9 248928      lvl2_longskip[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dec_lvl2b_conv (Conv3D)         (None, 48, 64, 42, 9 248928      dec_lvl2a_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_2to1 (Conv3DTranspose)  (None, 192, 256, 168 294960      dec_lvl2b_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lvl1_longskip (Add)             (None, 192, 256, 168 0           enc_lvl1a_conv[0][0]             \n",
      "                                                                 up_conv_2to1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dec_lvl1a_conv (Conv3D)         (None, 192, 256, 168 62256       lvl1_longskip[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Conv3D)            (None, 192, 256, 168 392         dec_lvl1a_conv[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 4,931,432\n",
      "Trainable params: 4,931,432\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "2019-10-10 16:19:03.272684\n"
     ]
    }
   ],
   "source": [
    "tb_log_dir = os.path.join('../output/tb_logs/' + dataset_name.split('/')[0])\n",
    "\n",
    "if not os.path.exists(tb_log_dir):    \n",
    "    os.makedirs(tb_log_dir)\n",
    "\n",
    "\n",
    "write_histogram = False\n",
    "\n",
    "if write_histogram == True:\n",
    "    tb_callback = utils.TrainValTensorBoard(model_name      = model_name,\n",
    "                                            write_graph     = True, \n",
    "                                            write_images    = False,\n",
    "                                            log_dir         = tb_log_dir,\n",
    "                                            histogram_freq  = 1,\n",
    "                                            batch_size      = 1,\n",
    "                                            )\n",
    "else:\n",
    "    tb_callback = utils.TrainValTensorBoard(model_name    = model_name,\n",
    "                                            write_graph   = True, \n",
    "                                            write_images  = False,\n",
    "                                            log_dir       = tb_log_dir,\n",
    "                                            )\n",
    "                                \n",
    "print(\"Model will be saved at the following location: %s\"%(model_path))          \n",
    "                                          \n",
    "checkpointer = ModelCheckpoint(model_path, verbose = 1, save_best_only = True)\n",
    "earlystopper = EarlyStopping(patience = 35, verbose = 1)\n",
    "\n",
    "# dimension of a single training volume (e.g. t1|ir|t2)\n",
    "input_data_dims = x_val.shape[1:]\n",
    "\n",
    "model = models_dict[model_arch](input_data_dims,\n",
    "                                num_segmask_labels,\n",
    "                                encoder_act_function = encoder_act_funct,\n",
    "                                decoder_act_function = decoder_act_funct,\n",
    "                                classification_act_function = classification_act_funct,\n",
    "                                loss_function = loss_funct,\n",
    "                                learning_rate = learning_rate,\n",
    "                                min_filters_per_layer = num_filters\n",
    "                                )\n",
    "\n",
    "\n",
    "# log model architecture on \"_ARCH.txt\" file\n",
    "with open(archfile_path, \"w\") as archfile:\n",
    "    model.summary(print_fn = lambda x: archfile.write(x + '\\n\\n')) \n",
    "\n",
    "\n",
    "# init log dictionary\n",
    "log_dict = dict()\n",
    "\n",
    "start_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "\n",
    "# update dictionary\n",
    "log_dict.update( dict(start_date        = start_date,\n",
    "                      model_name        = model_name,\n",
    "                      num_tr_samples    = training_set_size,\n",
    "                      tr_samples        = training_subdir_list,\n",
    "                      num_val_samples   = validation_set_size,\n",
    "                      val_samples       = validation_subdir_list,\n",
    "                      enc_act_funct     = encoder_act_funct,\n",
    "                      dec_act_funct     = decoder_act_funct,\n",
    "                      class_act_funct   = classification_act_funct,\n",
    "                      loss_funct        = loss_funct,\n",
    "                      lr                = learning_rate,\n",
    "                      num_filters       = num_filters,\n",
    "                      zscoring          = use_standardisation,\n",
    "                      zscoring_mean     = voxelwise_mean_path,\n",
    "                      zscoring_std      = voxelwise_std_path,\n",
    "                      )\n",
    "                )\n",
    "\n",
    "\n",
    "with open(logfile_path, \"w\") as logfile:\n",
    "    json.dump(log_dict, logfile, indent = 4, sort_keys = False)\n",
    "\n",
    "    \n",
    "\n",
    "# print time and some info in bash\n",
    "current_date_time = datetime.now()\n",
    "print (str(current_date_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we perform the actual training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 87/900 [=>............................] - ETA: 35:58 - loss: 1.0687 - categorical_crossentropy: 1.0687 - dice_coef_multilabel: 7.0612 - average_dice_coef_multilabel: 0.8826 - tanimoto_coefficient: 7.3069\n",
      " Warning: volume \"ASD25\" size mismatch: skipping to the next volume...\n",
      " 97/900 [==>...........................] - ETA: 35:13 - loss: 1.0164 - categorical_crossentropy: 1.0164 - dice_coef_multilabel: 7.0308 - average_dice_coef_multilabel: 0.8788 - tanimoto_coefficient: 7.2813\n",
      " Warning: volume \"AWA11\" size mismatch: skipping to the next volume...\n",
      "157/900 [====>.........................] - ETA: 31:40 - loss: 0.8154 - categorical_crossentropy: 0.8154 - dice_coef_multilabel: 6.8051 - average_dice_coef_multilabel: 0.8506 - tanimoto_coefficient: 7.1014\n",
      " Warning: volume \"CHA31\" size mismatch: skipping to the next volume...\n",
      "174/900 [====>.........................] - ETA: 30:49 - loss: 0.7799 - categorical_crossentropy: 0.7799 - dice_coef_multilabel: 6.7696 - average_dice_coef_multilabel: 0.8462 - tanimoto_coefficient: 7.0734\n",
      " Warning: volume \"CMT28\" size mismatch: skipping to the next volume...\n",
      "230/900 [======>.......................] - ETA: 28:09 - loss: 0.6914 - categorical_crossentropy: 0.6914 - dice_coef_multilabel: 6.6307 - average_dice_coef_multilabel: 0.8288 - tanimoto_coefficient: 6.9665\n",
      " Warning: volume \"DMH02\" size mismatch: skipping to the next volume...\n",
      "344/900 [==========>...................] - ETA: 23:06 - loss: 0.5943 - categorical_crossentropy: 0.5943 - dice_coef_multilabel: 6.4450 - average_dice_coef_multilabel: 0.8056 - tanimoto_coefficient: 6.8233\n",
      " Warning: volume \"HBS04\" size mismatch: skipping to the next volume...\n",
      "518/900 [================>.............] - ETA: 15:45 - loss: 0.5243 - categorical_crossentropy: 0.5243 - dice_coef_multilabel: 6.2719 - average_dice_coef_multilabel: 0.7840 - tanimoto_coefficient: 6.6892\n",
      " Warning: volume \"LGN18\" size mismatch: skipping to the next volume...\n",
      "528/900 [================>.............] - ETA: 15:20 - loss: 0.5200 - categorical_crossentropy: 0.5200 - dice_coef_multilabel: 6.2675 - average_dice_coef_multilabel: 0.7834 - tanimoto_coefficient: 6.6856\n",
      " Warning: volume \"LLG20\" size mismatch: skipping to the next volume...\n",
      "533/900 [================>.............] - ETA: 15:07 - loss: 0.5175 - categorical_crossentropy: 0.5175 - dice_coef_multilabel: 6.2683 - average_dice_coef_multilabel: 0.7835 - tanimoto_coefficient: 6.6862\n",
      " Warning: volume \"LMA30\" size mismatch: skipping to the next volume...\n",
      "567/900 [=================>............] - ETA: 13:43 - loss: 0.5048 - categorical_crossentropy: 0.5048 - dice_coef_multilabel: 6.2401 - average_dice_coef_multilabel: 0.7800 - tanimoto_coefficient: 6.6637\n",
      " Warning: volume \"MAL18\" size mismatch: skipping to the next volume...\n",
      "802/900 [=========================>....] - ETA: 4:01 - loss: 0.4451 - categorical_crossentropy: 0.4451 - dice_coef_multilabel: 6.0564 - average_dice_coef_multilabel: 0.7571 - tanimoto_coefficient: 6.5216\n",
      " Warning: volume \"SMA20\" size mismatch: skipping to the next volume...\n",
      "861/900 [===========================>..] - ETA: 1:35 - loss: 0.4310 - categorical_crossentropy: 0.4310 - dice_coef_multilabel: 6.0073 - average_dice_coef_multilabel: 0.7509 - tanimoto_coefficient: 6.4838\n",
      " Warning: volume \"VSN20\" size mismatch: skipping to the next volume...\n",
      "900/900 [==============================] - 2222s 2s/step - loss: 0.4235 - categorical_crossentropy: 0.4235 - dice_coef_multilabel: 5.9763 - average_dice_coef_multilabel: 0.7470 - tanimoto_coefficient: 6.4600 - val_loss: 0.2558 - val_categorical_crossentropy: 0.2558 - val_dice_coef_multilabel: 5.2277 - val_average_dice_coef_multilabel: 0.6535 - val_tanimoto_coefficient: 5.8980\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25584, saving model to ../output/models/glasgow/3T/c3rebrum_strconv_48f_900_cc.h5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "results = model.fit_generator(generator = train_generator,\n",
    "                              validation_data = (x_val, y_val),\n",
    "                              steps_per_epoch = training_samples,\n",
    "                              epochs = num_epochs,\n",
    "                              verbose = 1,\n",
    "                              callbacks = [checkpointer, tb_callback, earlystopper],\n",
    "                              max_queue_size = 1,\n",
    "                              use_multiprocessing = False,\n",
    "                              )\n",
    "\n",
    "\n",
    "# once the training finishes, log the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "log_dict.update( dict(elapsed_time = elapsed_time) )\n",
    "\n",
    "with open(logfile_path, \"w\") as logfile:\n",
    "    json.dump(log_dict, logfile, indent = 4, sort_keys = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training phase is finished, we can move to the testing notebook to see some results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('cerebrum': conda)",
   "language": "python",
   "name": "python37764bitcerebrumconda51d7ad0b476a4128bb4f71a16c353975"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}