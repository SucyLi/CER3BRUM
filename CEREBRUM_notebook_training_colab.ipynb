{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('cerebrum': conda)",
      "language": "python",
      "name": "python37764bitcerebrumconda51d7ad0b476a4128bb4f71a16c353975"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "colab": {
      "name": "CEREBRUM notebook - training colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SucyLi/CER3BRUM/blob/master/CEREBRUM_notebook_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2G6tNpv2BHz",
        "colab_type": "text"
      },
      "source": [
        "# CEREBRUM - iPython Notebook Demo (training)\n",
        "<br>\n",
        "Training of the fully-volumetric model introduced in:\n",
        "\n",
        "<b><font size=\"4\">[CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI](https://www.sciencedirect.com/science/article/pii/S1361841520300542)</font></b>\n",
        "\n",
        "\n",
        "Enjoy! 😃\n",
        "\n",
        "---\n",
        "\n",
        "This second notebook explains how to train a new instance of CEREBRUM.\n",
        "\n",
        "The training should require a total of 30-40 GBs of VRAM (depending on the size of the MR volume) when `batch_size = 1`. We trained the model exploiting a machine equipped with 4 Desktop GPUs (NVIDIA 1080Ti) - but similar performance can be achieved using a workstation with the required amount of VRAM (e.g., three-four NVIDIA 2080, two NVIDIA V100s, or one NVIDIA RTX 8000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ao39kt22EPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f7f78f29-b12d-477b-ddf5-72317931d232"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfenCaCE2BH0",
        "colab_type": "text"
      },
      "source": [
        "First, import all the libraries the notebook will be making use of:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjAwPnj2cd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "d8270aaa-3283-4a27-cd2c-7c322e8b9f77"
      },
      "source": [
        "import os\n",
        "os.chdir('/gdrive/My Drive/CMI/Ting/U-Net/CER3BRUM')\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: Keras>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.3.1)\n",
            "Requirement already satisfied: nibabel>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.18.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: tensorboard>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.8.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.5.0->-r requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.2->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.2->-r requirements.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.2->-r requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.7.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.6.0.post3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (0.34.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.9.0->-r requirements.txt (line 8)) (2.21.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.5.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.5.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.5.0->-r requirements.txt (line 2)) (0.1.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.9.0->-r requirements.txt (line 8)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.9.0->-r requirements.txt (line 8)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.9.0->-r requirements.txt (line 8)) (0.2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.9.0->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.9.0->-r requirements.txt (line 8)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.9.0->-r requirements.txt (line 8)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.9.0->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.9.0->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.9.0->-r requirements.txt (line 8)) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTPVU63V2BH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa02c4cd-3b11-4e9b-bcb1-e5ce3e524f6f"
      },
      "source": [
        "\n",
        "# specify the location of the library\n",
        "lib_path = '.'\n",
        "\n",
        "# garbage collection should prevent memory leaks during training exploiting generators\n",
        "import gc\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, lib_path)\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import json\n",
        "\n",
        "# useful for logging\n",
        "from datetime import datetime\n",
        "\n",
        "# import the CEREBRUM library\n",
        "os.chdir('/gdrive/My Drive/CMI/Ting/U-Net/CER3BRUM/src')\n",
        "from cer3brum_lib.vol_losses import losses_dict\n",
        "from cer3brum_lib import utils\n",
        "\n",
        "os.chdir('/gdrive/My Drive/CMI/Ting/U-Net/CER3BRUM/src/cer3brum_lib')\n",
        "print(os.getcwd)\n",
        "from fullyvol import cer3brum\n",
        "os.chdir('/gdrive/My Drive/CMI/Ting/U-Net/CER3BRUM/src')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in function getcwd>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUb1Bg7a2BH7",
        "colab_type": "text"
      },
      "source": [
        "There are a number of variables the user can tweak:\n",
        "\n",
        " - `data_dims` is a tuple containing the dimensions of the training and testing volumes.In our case, for instance, after the neck cropping, data are sized `(192, 256, 168)` (respectively, in sagittal, coronal, and longitudinal/axial view);\n",
        " \n",
        " - `num_segmask_labels` is an integer representing the number of different classes found in the ground truth data. In our work, as mentioned in the data section, we trained CEREBRUM to segment 8 different classes;\n",
        " \n",
        " - `dataset_name` is a string containing the name of the subfolder of `data` containing the dataset (training and validation;\n",
        " \n",
        " - `use_standardisation` is a boolean variable deciding whether or not input data should be z-scored (`True`) or not (`False`);\n",
        " \n",
        " - `anat_suffix` is a string/suffix identifying every anatomical volume (see the \"data\" Notebooks for a use case); \n",
        "\n",
        " - `segm_suffix` is a string/suffix identifying every ground truth volume (see the \"data\" Notebooks for a use case);\n",
        " \n",
        " - `num_epochs` is an integer specifying the number of epochs to train the model for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORrYEn3T2BH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tuple containing the dimension of each volume (tuple of three integers)\n",
        "data_dims = (256, 320, 320)\n",
        "\n",
        "# for sake of clarity in the following code, define:\n",
        "num_slices_sag = data_dims[0]\n",
        "num_slices_cor = data_dims[1]\n",
        "num_slices_lon = data_dims[2]\n",
        "\n",
        "# (int)\n",
        "num_segmask_labels = 7\n",
        "\n",
        "# (str)\n",
        "dataset_name = 'hcp'\n",
        "\n",
        "# (bool)\n",
        "use_standardisation = True\n",
        "\n",
        "# (str)\n",
        "anat_suffix = '_ANAT.nii.gz'\n",
        "\n",
        "# (str)\n",
        "segm_suffix = '_SEGM.nii.gz'\n",
        "\n",
        "# (int)\n",
        "num_epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXxf-CJz2BH-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "In addition, other parameters can be specified:\n",
        "\n",
        "- `learning_rate` is a float specifying the learning rate;\n",
        "- `num_filters` is an integer specifying the number of filters for the first layer of the architecture;\n",
        "- `encoder_act_funct` is a string specifying the activation function for the contraction path (encoder);\n",
        "- `decoder_act_funct` is a string specifying the activation function for the expansion path (decoder);\n",
        "- `loss_funct` is a string specifying the loss function to minimize during the training;\n",
        "- `model` is a string specifying the model architecture;\n",
        "- `training_samples` is an integer defining the number of samples (of the training set) to train the model instance on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srcXMXUg2BH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (float)\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# (int)\n",
        "num_filters = 48\n",
        "\n",
        "# (str - to choose from Keras activation functions: https://keras.io/activations/)\n",
        "encoder_act_funct = 'relu'\n",
        "\n",
        "# (str - to choose from Keras activation functions: https://keras.io/activations/)\n",
        "decoder_act_funct = 'relu'\n",
        "\n",
        "# (str - to choose between 'categorical_crossentropy', 'multiclass_dice_coeff',\n",
        "# 'weighted_multiclass_dice_coeff', 'tversky_loss', 'tanimoto_coefficient').\n",
        "loss_funct = 'categorical_crossentropy'\n",
        "\n",
        "# (str - to choose between 'strconv' and 'maxpool')\n",
        "model_arch = 'strconv'\n",
        "\n",
        "# (int)\n",
        "training_samples = 5\n",
        "\n",
        "# (int)\n",
        "val_samples = 1\n",
        "\n",
        "# (str)\n",
        "classification_act_funct = 'softmax'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlscfkQz2BIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# change this to match your dataset location\n",
        "data_path = os.path.join('../data', dataset_name)\n",
        "\n",
        "# specify the folders containing the training set and the validation set\n",
        "training_path = os.path.join(data_path, 'training')\n",
        "validation_path = os.path.join(data_path, 'validation')\n",
        "\n",
        "# build a model dictionary (for sake of clarity of the user-defined parameters)\n",
        "models_dict = {'maxpool' : cer3brum.ThreeLevelsMaxPool,\n",
        "               'strconv' : cer3brum.ThreeLevelsStrConv,\n",
        "              }\n",
        "               \n",
        "names_dict = {'maxpool' : 'c3rebrum_maxpool_%df_%d'%(num_filters, training_samples),\n",
        "              'strconv' : 'c3rebrum_strconv_%df_%d'%(num_filters, training_samples),\n",
        "              }\n",
        "\n",
        "losses_names_dict = {'categorical_crossentropy'       : '_cc',\n",
        "                     'multiclass_dice_coeff'          : '_sum_dc',\n",
        "                     'average_multiclass_dice_coeff'  : '_avg_dc',\n",
        "                     'weighted_multiclass_dice_coeff' : '_wght_dc',\n",
        "                     'tversky_loss'                   : '_tsv',\n",
        "                     'tanimoto_coefficient'           : '_tc'}\n",
        "\n",
        "# the model name is function of the chosen hyperparameters\n",
        "model_name = names_dict[model_arch] + losses_names_dict[loss_funct] + '.h5'\n",
        "\n",
        "models_dir  = os.path.join('../output/models/', dataset_name)\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "\n",
        "logs_dir = os.path.join('../output/logs', dataset_name)\n",
        "if not os.path.exists(logs_dir):\n",
        "    os.makedirs(logs_dir)\n",
        "         \n",
        "model_path    = os.path.join(models_dir, model_name)\n",
        "\n",
        "# location of the \"_LOG.json\" file - where all the training param.s will be logged\n",
        "logfile_path  = os.path.join(logs_dir, model_name[0:-3] + '_LOG.json')\n",
        "\n",
        "# location of the \"_ARCH.txt\" file - where the output of Kerad model.summary() will be saved\n",
        "archfile_path = os.path.join(logs_dir, model_name[0:-3] + '_ARCH.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wiNGHUH2BIC",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "After building the lists of volumes we're going to train and validate on, load the z-score matrices computed in the data notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sk24I8v2BID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61d6f88d-ad22-47cd-b8d4-c0db3599c7c2"
      },
      "source": [
        "# training directory and validation directory structure (one anatomical + segmentation per subdir)\n",
        "training_subdir_list = list() \n",
        "training_volumes_path = list()\n",
        "training_subdir_list = sorted(os.listdir(training_path))\n",
        "training_set_size = len(training_subdir_list)\n",
        "for idx in training_subdir_list:\n",
        "    training_volumes_path.append(os.path.join(training_path, idx))\n",
        "\n",
        "validation_subdir_list = list() \n",
        "validation_volumes_path = list()\n",
        "validation_subdir_list = sorted(os.listdir(validation_path))[0:val_samples]\n",
        "validation_set_size = len(validation_subdir_list)\n",
        "for idx in validation_subdir_list:\n",
        "    validation_volumes_path.append(os.path.join(validation_path, idx))\n",
        "\n",
        "print('Training on %d volumes, validating on %d (%s)'%(training_set_size, validation_set_size, dataset_name))\n",
        "\n",
        "\n",
        "\n",
        "stdz_matrices_path = os.path.join('../output/zscoring/', dataset_name)\n",
        "\n",
        "# in this case, mean and std are computed on the first 250 training volumes of the dataset\n",
        "voxelwise_mean_path = os.path.join(stdz_matrices_path, 'voxelwise_mean.nii.gz')\n",
        "voxelwise_std_path = os.path.join(stdz_matrices_path, 'voxelwise_std.nii.gz')\n",
        "\n",
        "if not os.path.exists(voxelwise_mean_path) or not os.path.exists(voxelwise_std_path):\n",
        "    print(\"ERROR: voxelwise mean and standard deviation volumes not found.\")\n",
        "    print('Have you run the data notebook?')    \n",
        "    sys.exit(0)\n",
        "\n",
        "# to make the data suitable for pooling, reduce the last dimension so that it can be divided by 2^3\n",
        "voxelwise_mean = np.array(nib.load(voxelwise_mean_path).dataobj[:, :, :]).astype(dtype = 'float32')\n",
        "voxelwise_std = np.array(nib.load(voxelwise_std_path).dataobj[:, :, :]).astype(dtype = 'float32')\n",
        "\n",
        "# zero-padding is added by some pipelines at the borders, thus the voxelwise variability\n",
        "# would result zero. Change these values to 1 in order to mantain the zero in the division\n",
        "# (and not give rise to NaNs)\n",
        "voxelwise_std[voxelwise_std == 0] = 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 5 volumes, validating on 1 (hcp)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q1wXrAd2BIF",
        "colab_type": "text"
      },
      "source": [
        "In order to train on an arbitrary number of volumes, we build a generator that handles the loading of such data dinamically (not saturating the RAM of the machine we're using). In addition, we define a couple of useful function (for sake of clarity):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aKut1y62BIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check volume integrity (presence of NaN and inf could arise during the standardisation)\n",
        "def check_vol_integrity(input_vol, vol_name):\n",
        "\n",
        "    if np.sum(np.isnan(input_vol)) != 0:\n",
        "        print('WARNING: %d NaN(s) found in volume \"%s\"!'%(np.sum(np.isnan(input_vol)), vol_name))\n",
        "        sys.exit(0)\n",
        "        \n",
        "    if np.sum(np.isinf(input_vol)) != 0:\n",
        "        print('WARNING: %d inf(s) found in volume \"%s\"!'%(np.sum(np.isinf(input_vol)), vol_name))\n",
        "        sys.exit(0)\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# volumes z-scoring \n",
        "def volume_zscoring(input_vol, voxelwise_mean, voxelwise_std):\n",
        "\n",
        "    # standardize each training volume\n",
        "    input_vol -= voxelwise_mean\n",
        "    input_vol /= voxelwise_std\n",
        "    \n",
        "    return input_vol\n",
        "\n",
        "## ----------------------------------------\n",
        "\n",
        "# create a generator that handles data loading dynamically in order to fit the entire dataset into RAM\n",
        "def vol_generator(vol_list, data_dims, num_segmask_labels, batch_size,\n",
        "                  use_standardisation, voxelwise_mean, voxelwise_std):\n",
        "    \n",
        "    # \"batch_size\" here is not the \"actual\" batch size (onto which the gradient is computed),\n",
        "    # but just the number of volumes to load dinamically at once\n",
        "    num_volumes = len(vol_list)\n",
        "    \n",
        "    anat_suffix = '_ANAT.nii.gz'\n",
        "    segm_suffix = '_SEGM.nii.gz'\n",
        "    \n",
        "    while 1: \n",
        "\n",
        "        shuffle(vol_list)\n",
        "\n",
        "        # if \"num_samples\" cannot be divided without remainder by \"batch_size\", some samples are discarded\n",
        "        # (at mostr batch_size - 1); if batch_size = 1 (as in the fully volumetric case) then all the dataset\n",
        "        # is loaded during the execution\n",
        "        for offset in range(0, num_volumes, batch_size):\n",
        "            \n",
        "            # define the batch that will be loaded when the yield instruction is executed (vol paths)\n",
        "            batch_volumes_list = vol_list[offset : offset+batch_size]\n",
        "\n",
        "            # init the list that will contain the samples\n",
        "            x_train_batch = np.zeros((batch_size, data_dims[0], data_dims[1], data_dims[2], 1), dtype=np.float)\n",
        "            y_train_batch = np.zeros((batch_size, data_dims[0], data_dims[1], data_dims[2], num_segmask_labels), dtype=np.float)\n",
        "\n",
        "            # for every sample in the selected chunk\n",
        "            for vol_num, vol in enumerate(batch_volumes_list):\n",
        "                \n",
        "                vol_name = vol.split('/')[-1]\n",
        "\n",
        "                mri_path = os.path.join(vol, vol_name + anat_suffix)\n",
        "                segmask_path = os.path.join(vol, vol_name + segm_suffix)  \n",
        "\n",
        "                # load the actual volume cropping to 168 in the last dimension to avoid problems during pooling\n",
        "                temp = np.array(nib.load(mri_path).dataobj[:, :, :]).astype(dtype = 'float32')\n",
        "\n",
        "                # check if everything is ok\n",
        "                if temp.shape != data_dims:\n",
        "                    print('\\n Warning: volume \"%s\" size mismatch: skipping to the next volume...'%(vol_name))\n",
        "                    continue\n",
        "                \n",
        "                if use_standardisation == True:\n",
        "                    temp = volume_zscoring(temp, voxelwise_mean, voxelwise_std)\n",
        "\n",
        "                check_vol_integrity(temp, vol_name)\n",
        "\n",
        "                x_train_batch[vol_num, :, :, :, 0] = temp\n",
        "\n",
        "                seg = np.array(nib.load(segmask_path).dataobj[:, :, :]).astype(dtype = 'uint8')\n",
        "        \n",
        "                # #  as we don't have many cases of white matter lesions (class 4) merge this class with white matter (class 3\n",
        "                # seg_vol_fixed = np.copy(seg)\n",
        "                # seg_vol_fixed[seg_vol_fixed==4] = 3\n",
        "\n",
        "                # # \"shift\" every class by one (as class 4 is now empty)\n",
        "                # seg_vol_fixed[seg_vol_fixed>4]-=1\n",
        "        \n",
        "                y_train_batch[vol_num] = np_utils.to_categorical(seg, num_segmask_labels)\n",
        "\n",
        "            # final batch-wise shuffling (meaningful iff batch_size>1)\n",
        "            yield shuffle(x_train_batch, y_train_batch)\n",
        "\n",
        "            gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9uDIiX82BIH",
        "colab_type": "text"
      },
      "source": [
        "After the generator is defined, initialise it (dynamic loading of training data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzheERjN2BII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_sequences = 1\n",
        "\n",
        "train_generator = vol_generator(vol_list            = training_volumes_path,\n",
        "                                data_dims           = data_dims,\n",
        "                                num_segmask_labels  = num_segmask_labels,\n",
        "                                batch_size          = 1,\n",
        "                                use_standardisation = use_standardisation,\n",
        "                                voxelwise_mean      = voxelwise_mean,\n",
        "                                voxelwise_std       = voxelwise_std,\n",
        "                                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1vPEQu2BIK",
        "colab_type": "text"
      },
      "source": [
        "Given that validation volumes are less than training volumes in number, we choose to load them statically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEnQDrnv2BIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e011cee4-9a7e-490d-85eb-aa8cc6dada23"
      },
      "source": [
        "# define the validation data as a 5D volumes: VOLNUMxDIM1xDIM2xDIM3xCHs\n",
        "x_val = np.zeros((validation_set_size, num_slices_sag, num_slices_cor, num_slices_lon, num_sequences),\n",
        "                 dtype=np.float)\n",
        "\n",
        "# define the validation GT as a 5D volumes: VOLNUMxDIM1xDIM2xDIM3xN_CLASSES\n",
        "y_val = np.zeros((validation_set_size, num_slices_sag, num_slices_cor, num_slices_lon, num_segmask_labels),\n",
        "                 dtype=np.uint8)\n",
        "\n",
        "vol_num = 0\n",
        "\n",
        "\n",
        "for vol in validation_volumes_path:\n",
        "\n",
        "    vol_name = vol.split('/')[-1]\n",
        "\n",
        "    mri_path = os.path.join(vol, vol_name + anat_suffix)\n",
        "    segmask_path = os.path.join(vol, vol_name + segm_suffix)\n",
        "\n",
        "    print('(%03d/%d) - loading validation volume %s and its mask...'%(vol_num+1, validation_set_size, vol_name)),\n",
        "\n",
        "    # to make the data suitable for pooling, reduce the last dimension so that it can be divided by 2^3\n",
        "    temp = np.array(nib.load(mri_path).dataobj[:, :, :]).astype(dtype = 'float32')\n",
        "    \n",
        "    # check if everything is ok\n",
        "    if temp.shape != data_dims:\n",
        "        print('\\n Warning: volume \"%s\" size mismatch: skipping to the next volume...'%(vol_name))\n",
        "        continue\n",
        "\n",
        "    if use_standardisation == True:\n",
        "        temp = volume_zscoring(temp, voxelwise_mean, voxelwise_std)\n",
        "    \n",
        "    check_vol_integrity(temp, vol_name)\n",
        "\n",
        "    x_val[vol_num, :, :, :, 0] = temp\n",
        "\n",
        "\n",
        "    seg = np.array(nib.load(segmask_path).dataobj[:, :, :]).astype(dtype = 'uint8')\n",
        "\n",
        "    #  as we don't have many cases of white matter lesions (class 4) merge this class with white matter (class 3\n",
        "    # seg_vol_fixed = np.copy(seg)\n",
        "    # seg_vol_fixed[seg_vol_fixed==4] = 3\n",
        "    \n",
        "    # \"shift\" every class by one (as class 4 is now empty)\n",
        "    # seg_vol_fixed[seg_vol_fixed>4]-=1\n",
        "    \n",
        "    y_val[vol_num] = np_utils.to_categorical(seg, num_segmask_labels)\n",
        "    \n",
        "    vol_num = vol_num + 1\n",
        "\n",
        "    print('Done.')\n",
        "\n",
        "# to check if the class merge operation worked out as intended, make sure each voxel was\n",
        "# assigned to exactly one class\n",
        "# assert np.min(np.sum(y_val, axis = 4)) == 1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(001/1) - loading validation volume VALVOL1 and its mask...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBNKDoMK2BIM",
        "colab_type": "text"
      },
      "source": [
        "We then initialise the model, define some callbacks, and take care of some of the logging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0UVgut02BIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "856761a4-d523-44e3-e781-b14b3f4c5385"
      },
      "source": [
        "tb_log_dir = os.path.join('../output/tb_logs/' + dataset_name.split('/')[0])\n",
        "\n",
        "if not os.path.exists(tb_log_dir):    \n",
        "    os.makedirs(tb_log_dir)\n",
        "\n",
        "\n",
        "write_histogram = False\n",
        "\n",
        "if write_histogram == True:\n",
        "    tb_callback = utils.TrainValTensorBoard(model_name      = model_name,\n",
        "                                            write_graph     = True, \n",
        "                                            write_images    = False,\n",
        "                                            log_dir         = tb_log_dir,\n",
        "                                            histogram_freq  = 1,\n",
        "                                            batch_size      = 1,\n",
        "                                            )\n",
        "else:\n",
        "    tb_callback = utils.TrainValTensorBoard(model_name    = model_name,\n",
        "                                            write_graph   = True, \n",
        "                                            write_images  = False,\n",
        "                                            log_dir       = tb_log_dir,\n",
        "                                            )\n",
        "                                \n",
        "print(\"Model will be saved at the following location: %s\"%(model_path))          \n",
        "                                          \n",
        "checkpointer = ModelCheckpoint(model_path, verbose = 1, save_best_only = True)\n",
        "earlystopper = EarlyStopping(patience = 35, verbose = 1)\n",
        "\n",
        "# dimension of a single training volume (e.g. t1|ir|t2)\n",
        "input_data_dims = x_val.shape[1:]\n",
        "\n",
        "model = models_dict[model_arch](input_data_dims,\n",
        "                                num_segmask_labels,\n",
        "                                encoder_act_function = encoder_act_funct,\n",
        "                                decoder_act_function = decoder_act_funct,\n",
        "                                classification_act_function = classification_act_funct,\n",
        "                                loss_function = loss_funct,\n",
        "                                learning_rate = learning_rate,\n",
        "                                min_filters_per_layer = num_filters\n",
        "                                )\n",
        "\n",
        "\n",
        "# log model architecture on \"_ARCH.txt\" file\n",
        "with open(archfile_path, \"w\") as archfile:\n",
        "    model.summary(print_fn = lambda x: archfile.write(x + '\\n\\n')) \n",
        "\n",
        "\n",
        "# init log dictionary\n",
        "log_dict = dict()\n",
        "\n",
        "start_date = datetime.now().strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
        "\n",
        "# update dictionary\n",
        "log_dict.update( dict(start_date        = start_date,\n",
        "                      model_name        = model_name,\n",
        "                      num_tr_samples    = training_set_size,\n",
        "                      tr_samples        = training_subdir_list,\n",
        "                      num_val_samples   = validation_set_size,\n",
        "                      val_samples       = validation_subdir_list,\n",
        "                      enc_act_funct     = encoder_act_funct,\n",
        "                      dec_act_funct     = decoder_act_funct,\n",
        "                      class_act_funct   = classification_act_funct,\n",
        "                      loss_funct        = loss_funct,\n",
        "                      lr                = learning_rate,\n",
        "                      num_filters       = num_filters,\n",
        "                      zscoring          = use_standardisation,\n",
        "                      zscoring_mean     = voxelwise_mean_path,\n",
        "                      zscoring_std      = voxelwise_std_path,\n",
        "                      )\n",
        "                )\n",
        "\n",
        "\n",
        "with open(logfile_path, \"w\") as logfile:\n",
        "    json.dump(log_dict, logfile, indent = 4, sort_keys = False)\n",
        "\n",
        "\n",
        "# print time and some info in bash\n",
        "current_date_time = datetime.now()\n",
        "print (str(current_date_time))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model will be saved at the following location: ../output/models/hcp/c3rebrum_strconv_48f_5_cc.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: slice index 7 of dimension 4 out of bounds. for '{{node metrics_1/average_dice_coef_multilabel/strided_slice_16}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=15, ellipsis_mask=0, end_mask=15, new_axis_mask=0, shrink_axis_mask=16](main_output_1/truediv, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack_1, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack_2)' with input shapes: [?,256,320,320,7], [5], [5], [5] and with computed input tensors: input[1] = <0 0 0 0 7>, input[2] = <0 0 0 0 8>, input[3] = <1 1 1 1 1>.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6b3ec7154ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                                 \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_funct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                 \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                 \u001b[0mmin_filters_per_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                                 )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m./fullyvol/cer3brum.py\u001b[0m in \u001b[0;36mThreeLevelsStrConv\u001b[0;34m(input_dims, num_classes, init, encoder_act_function, decoder_act_function, classification_act_function, loss_function, learning_rate, min_filters_per_layer, use_kernel_reg, use_dropout)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mskip_target_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             masks=masks)\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Compute total loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_metrics\u001b[0;34m(self, outputs, targets, skip_target_masks, sample_weights, masks)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                 self._handle_per_output_metrics(\n\u001b[0;32m--> 871\u001b[0;31m                     self._per_output_metrics[i], target, output, output_mask)\n\u001b[0m\u001b[1;32m    872\u001b[0m                 self._handle_per_output_metrics(\n\u001b[1;32m    873\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_per_output_weighted_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_handle_per_output_metrics\u001b[0;34m(self, metrics_dict, y_true, y_pred, mask, weights)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 training_utils.call_metric_function(\n\u001b[0;32m--> 842\u001b[0;31m                     metric_fn, y_true, y_pred, weights=weights, mask=mask)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     def _handle_metrics(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcall_metric_function\u001b[0;34m(metric_fn, y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For TF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mmetric_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"\"\"Decorated function with `add_update()`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_or_expand_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         return super(MeanMetricWrapper, self).update_state(\n\u001b[1;32m    320\u001b[0m             matches, sample_weight=sample_weight)\n",
            "\u001b[0;32m/gdrive/My Drive/CMI/Ting/U-Net/CER3BRUM/src/cer3brum_lib/vol_losses.py\u001b[0m in \u001b[0;36maverage_dice_coef_multilabel\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# average multiclass dice coefficient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mmulticlass_dice\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mdice_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmulticlass_dice\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1148\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10177\u001b[0m                         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10178\u001b[0m                         \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10179\u001b[0;31m                         shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m  10180\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10181\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3325\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3326\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3327\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3328\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0;32m-> 1817\u001b[0;31m                                 control_input_ops, op_def)\n\u001b[0m\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: slice index 7 of dimension 4 out of bounds. for '{{node metrics_1/average_dice_coef_multilabel/strided_slice_16}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, begin_mask=15, ellipsis_mask=0, end_mask=15, new_axis_mask=0, shrink_axis_mask=16](main_output_1/truediv, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack_1, metrics_1/average_dice_coef_multilabel/strided_slice_16/stack_2)' with input shapes: [?,256,320,320,7], [5], [5], [5] and with computed input tensors: input[1] = <0 0 0 0 7>, input[2] = <0 0 0 0 8>, input[3] = <1 1 1 1 1>."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l0wfdWQ2BIP",
        "colab_type": "text"
      },
      "source": [
        "Then we perform the actual training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCVfIx6V2BIQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "6d07632e-6624-443d-9953-25972474fa2c"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "results = model.fit_generator(generator = train_generator,\n",
        "                              validation_data = (x_val, y_val),\n",
        "                              steps_per_epoch = training_samples,\n",
        "                              epochs = num_epochs,\n",
        "                              verbose = 1,\n",
        "                              callbacks = [checkpointer, tb_callback, earlystopper],\n",
        "                              max_queue_size = 1,\n",
        "                              use_multiprocessing = False,\n",
        "                              )\n",
        "\n",
        "\n",
        "# once the training finishes, log the elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "log_dict.update( dict(elapsed_time = elapsed_time) )\n",
        "\n",
        "with open(logfile_path, \"w\") as logfile:\n",
        "    json.dump(log_dict, logfile, indent = 4, sort_keys = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            " 87/900 [=>............................] - ETA: 35:58 - loss: 1.0687 - categorical_crossentropy: 1.0687 - dice_coef_multilabel: 7.0612 - average_dice_coef_multilabel: 0.8826 - tanimoto_coefficient: 7.3069\n",
            " Warning: volume \"ASD25\" size mismatch: skipping to the next volume...\n",
            " 97/900 [==>...........................] - ETA: 35:13 - loss: 1.0164 - categorical_crossentropy: 1.0164 - dice_coef_multilabel: 7.0308 - average_dice_coef_multilabel: 0.8788 - tanimoto_coefficient: 7.2813\n",
            " Warning: volume \"AWA11\" size mismatch: skipping to the next volume...\n",
            "157/900 [====>.........................] - ETA: 31:40 - loss: 0.8154 - categorical_crossentropy: 0.8154 - dice_coef_multilabel: 6.8051 - average_dice_coef_multilabel: 0.8506 - tanimoto_coefficient: 7.1014\n",
            " Warning: volume \"CHA31\" size mismatch: skipping to the next volume...\n",
            "174/900 [====>.........................] - ETA: 30:49 - loss: 0.7799 - categorical_crossentropy: 0.7799 - dice_coef_multilabel: 6.7696 - average_dice_coef_multilabel: 0.8462 - tanimoto_coefficient: 7.0734\n",
            " Warning: volume \"CMT28\" size mismatch: skipping to the next volume...\n",
            "230/900 [======>.......................] - ETA: 28:09 - loss: 0.6914 - categorical_crossentropy: 0.6914 - dice_coef_multilabel: 6.6307 - average_dice_coef_multilabel: 0.8288 - tanimoto_coefficient: 6.9665\n",
            " Warning: volume \"DMH02\" size mismatch: skipping to the next volume...\n",
            "344/900 [==========>...................] - ETA: 23:06 - loss: 0.5943 - categorical_crossentropy: 0.5943 - dice_coef_multilabel: 6.4450 - average_dice_coef_multilabel: 0.8056 - tanimoto_coefficient: 6.8233\n",
            " Warning: volume \"HBS04\" size mismatch: skipping to the next volume...\n",
            "518/900 [================>.............] - ETA: 15:45 - loss: 0.5243 - categorical_crossentropy: 0.5243 - dice_coef_multilabel: 6.2719 - average_dice_coef_multilabel: 0.7840 - tanimoto_coefficient: 6.6892\n",
            " Warning: volume \"LGN18\" size mismatch: skipping to the next volume...\n",
            "528/900 [================>.............] - ETA: 15:20 - loss: 0.5200 - categorical_crossentropy: 0.5200 - dice_coef_multilabel: 6.2675 - average_dice_coef_multilabel: 0.7834 - tanimoto_coefficient: 6.6856\n",
            " Warning: volume \"LLG20\" size mismatch: skipping to the next volume...\n",
            "533/900 [================>.............] - ETA: 15:07 - loss: 0.5175 - categorical_crossentropy: 0.5175 - dice_coef_multilabel: 6.2683 - average_dice_coef_multilabel: 0.7835 - tanimoto_coefficient: 6.6862\n",
            " Warning: volume \"LMA30\" size mismatch: skipping to the next volume...\n",
            "567/900 [=================>............] - ETA: 13:43 - loss: 0.5048 - categorical_crossentropy: 0.5048 - dice_coef_multilabel: 6.2401 - average_dice_coef_multilabel: 0.7800 - tanimoto_coefficient: 6.6637\n",
            " Warning: volume \"MAL18\" size mismatch: skipping to the next volume...\n",
            "802/900 [=========================>....] - ETA: 4:01 - loss: 0.4451 - categorical_crossentropy: 0.4451 - dice_coef_multilabel: 6.0564 - average_dice_coef_multilabel: 0.7571 - tanimoto_coefficient: 6.5216\n",
            " Warning: volume \"SMA20\" size mismatch: skipping to the next volume...\n",
            "861/900 [===========================>..] - ETA: 1:35 - loss: 0.4310 - categorical_crossentropy: 0.4310 - dice_coef_multilabel: 6.0073 - average_dice_coef_multilabel: 0.7509 - tanimoto_coefficient: 6.4838\n",
            " Warning: volume \"VSN20\" size mismatch: skipping to the next volume...\n",
            "900/900 [==============================] - 2222s 2s/step - loss: 0.4235 - categorical_crossentropy: 0.4235 - dice_coef_multilabel: 5.9763 - average_dice_coef_multilabel: 0.7470 - tanimoto_coefficient: 6.4600 - val_loss: 0.2558 - val_categorical_crossentropy: 0.2558 - val_dice_coef_multilabel: 5.2277 - val_average_dice_coef_multilabel: 0.6535 - val_tanimoto_coefficient: 5.8980\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.25584, saving model to ../output/models/glasgow/3T/c3rebrum_strconv_48f_900_cc.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQXm34cd2BIS",
        "colab_type": "text"
      },
      "source": [
        "Once the training phase is finished, we can move to the testing notebook to see some results."
      ]
    }
  ]
}